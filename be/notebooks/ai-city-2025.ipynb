{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2988f328",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:39:28.681585Z",
     "iopub.status.busy": "2025-04-03T08:39:28.681388Z",
     "iopub.status.idle": "2025-04-03T08:40:05.299625Z",
     "shell.execute_reply": "2025-04-03T08:40:05.298547Z"
    },
    "papermill": {
     "duration": 36.627865,
     "end_time": "2025-04-03T08:40:05.301356",
     "exception": false,
     "start_time": "2025-04-03T08:39:28.673491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\r\n",
      "  Downloading ultralytics-8.3.100-py3-none-any.whl.metadata (37 kB)\r\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\r\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\r\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\r\n",
      "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\r\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\r\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\r\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\r\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\r\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\r\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\r\n",
      "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\r\n",
      "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\r\n",
      "Downloading ultralytics-8.3.100-py3-none-any.whl (977 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m977.1/977.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\r\n",
      "Installing collected packages: ultralytics-thop, ultralytics\r\n",
      "Successfully installed ultralytics-8.3.100 ultralytics-thop-2.0.14\r\n",
      "Collecting flash-attn\r\n",
      "  Downloading flash_attn-2.7.4.post1.tar.gz (6.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.12.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\r\n",
      "Building wheels for collected packages: flash-attn\r\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for flash-attn: filename=flash_attn-2.7.4.post1-cp310-cp310-linux_x86_64.whl size=187797312 sha256=b267f80a08e516292cdd748056a2178a45b8abedf7fca123292eb17c21c8c87c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/ce/d5/08ea07bfc16ba218dc65a3a7ef9b6a270530bcbd2cea2ee1ca\r\n",
      "Successfully built flash-attn\r\n",
      "Installing collected packages: flash-attn\r\n",
      "Successfully installed flash-attn-2.7.4.post1\r\n",
      "Collecting argparse\r\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\r\n",
      "Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\r\n",
      "Installing collected packages: argparse\r\n",
      "Successfully installed argparse-1.4.0\r\n",
      "Collecting yacs\r\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\r\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs) (6.0.2)\r\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\r\n",
      "Installing collected packages: yacs\r\n",
      "Successfully installed yacs-0.1.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics\n",
    "!pip install flash-attn\n",
    "!pip install argparse\n",
    "!pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f62281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:05.318840Z",
     "iopub.status.busy": "2025-04-03T08:40:05.318599Z",
     "iopub.status.idle": "2025-04-03T08:40:05.323039Z",
     "shell.execute_reply": "2025-04-03T08:40:05.322451Z"
    },
    "papermill": {
     "duration": 0.014108,
     "end_time": "2025-04-03T08:40:05.324190",
     "exception": false,
     "start_time": "2025-04-03T08:40:05.310082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "def download_file(path, download_file_name):\n",
    "    os.chdir('/kaggle/working/')\n",
    "    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n",
    "    command = f\"zip {zip_name} {path} -r\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Unable to run zip command!\")\n",
    "        print(result.stderr)\n",
    "        return\n",
    "    display(FileLink(f'{download_file_name}.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc94c77",
   "metadata": {
    "papermill": {
     "duration": 0.007616,
     "end_time": "2025-04-03T08:40:05.339844",
     "exception": false,
     "start_time": "2025-04-03T08:40:05.332228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1.  Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb4d8ed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:05.356679Z",
     "iopub.status.busy": "2025-04-03T08:40:05.356420Z",
     "iopub.status.idle": "2025-04-03T08:40:05.360892Z",
     "shell.execute_reply": "2025-04-03T08:40:05.360111Z"
    },
    "papermill": {
     "duration": 0.014429,
     "end_time": "2025-04-03T08:40:05.362227",
     "exception": false,
     "start_time": "2025-04-03T08:40:05.347798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# from glob import glob\n",
    "# from ultralytics import YOLO\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from IPython.display import clear_output   #only jupyter notebook\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# scene_path = \"/kaggle/input/dataset-acc2024/scene_039/\"\n",
    "# output_text_dir = \"output_text/\"\n",
    "# os.makedirs(output_text_dir, exist_ok=True)\n",
    "\n",
    "# # Load YOLOv11 model\n",
    "# model = YOLO('/kaggle/input/yolov11l/pytorch/default/1/yolo11l.pt').to(device)\n",
    "# class_names = model.model.names  \n",
    "\n",
    "# camera_folders = glob(os.path.join(scene_path, \"camera_*\"))\n",
    "# print(\"Found camera folders:\", camera_folders)  \n",
    "\n",
    "# def track_persons(detections, person_ids, frame_id):\n",
    "#     person_detections = [d for d in detections if d[5] == 0]  \n",
    "#     tracked = []\n",
    "\n",
    "#     for det in person_detections:\n",
    "#         x1, y1, x2, y2, score, _, _ = det \n",
    "#         person_id = None\n",
    "        \n",
    "#         for pid, last_det in person_ids.items():\n",
    "#             last_x1, last_y1, last_x2, last_y2 = last_det[:4]\n",
    "#             iou = compute_iou((x1, y1, x2, y2), (last_x1, last_y1, last_x2, last_y2))\n",
    "#             if iou > 0.5:\n",
    "#                 person_id = pid\n",
    "#                 break\n",
    "        \n",
    "#         if person_id is None:\n",
    "#             person_id = len(person_ids) + 1\n",
    "        \n",
    "#         tracked.append((person_id, x1, y1, x2, y2, score))\n",
    "#         person_ids[person_id] = (x1, y1, x2, y2, score)\n",
    "    \n",
    "#     return tracked, person_ids\n",
    "    \n",
    "# def compute_iou(box1, box2):\n",
    "#     x1, y1, x2, y2 = box1\n",
    "#     xx1, yy1, xx2, yy2 = box2\n",
    "\n",
    "#     inter_x1 = max(x1, xx1)\n",
    "#     inter_y1 = max(y1, yy1)\n",
    "#     inter_x2 = min(x2, xx2)\n",
    "#     inter_y2 = min(y2, yy2)\n",
    "\n",
    "#     inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)\n",
    "#     area1 = (x2 - x1) * (y2 - y1)\n",
    "#     area2 = (xx2 - xx1) * (yy2 - yy1)\n",
    "\n",
    "#     iou = inter_area / (area1 + area2 - inter_area)\n",
    "#     return iou\n",
    "\n",
    "# count_folder = 0\n",
    "# count_frame = []\n",
    "# for cam_folder in camera_folders:\n",
    "#     video_path = os.path.join(cam_folder, \"video.mp4\")\n",
    "#     if not os.path.exists(video_path):\n",
    "#         continue\n",
    "    \n",
    "#     cam_id = os.path.basename(cam_folder).split('_')[1]\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "#     if not cap.isOpened():\n",
    "#         continue  # Skip unopenable videos\n",
    "    \n",
    "#     frame_count = 0  # Reset for each camera\n",
    "#     cam_name = os.path.basename(cam_folder)\n",
    "#     cam_text_output_dir = os.path.join(output_text_dir, cam_name)\n",
    "#     os.makedirs(cam_text_output_dir, exist_ok=True)\n",
    "    \n",
    "#     text_file_path = os.path.join(cam_text_output_dir, \"detections.txt\")\n",
    "#     person_ids = {}\n",
    "#     count_frame.append(frame_count)\n",
    "#     with open(text_file_path, \"w\") as f:\n",
    "#         f.write(\"cam_id,frame_id,person_id,x1,y1,x2,y2,score\\n\")\n",
    "        \n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if frame_count >= 300:\n",
    "#                 break  # Stop at 300 frames or video end\n",
    "            \n",
    "#             # Process the frame\n",
    "#             results = model(frame)\n",
    "#             clear_output(wait=True)\n",
    "#             detections = []\n",
    "#             for r in results:\n",
    "#                 for box in r.boxes:\n",
    "#                     x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "#                     conf = box.conf[0].item()\n",
    "#                     cls = int(box.cls[0].item())\n",
    "#                     if cls == 0:  # Person class\n",
    "#                         detections.append([x1, y1, x2, y2, conf, cls, frame_count])\n",
    "            \n",
    "#             tracked_detections, person_ids = track_persons(detections, person_ids, frame_count)\n",
    "            \n",
    "#             # Write data with current frame_count (0–299)\n",
    "#             for det in tracked_detections:\n",
    "#                 person_id, x1, y1, x2, y2, score = det\n",
    "#                 f.write(f\"{cam_id},{frame_count},{person_id},{x1},{y1},{x2},{y2},{score:.2f}\\n\")\n",
    "            \n",
    "#             frame_count += 1  # Increment AFTER processing\n",
    "#     count_frame.append(frame_count)\n",
    "#     count_folder +=1        \n",
    "#     cap.release()\n",
    "# print(count_folder)\n",
    "# print(\"Person ID tracking and bounding box extraction completed for all cameras!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a0410f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:05.378661Z",
     "iopub.status.busy": "2025-04-03T08:40:05.378440Z",
     "iopub.status.idle": "2025-04-03T08:40:05.381467Z",
     "shell.execute_reply": "2025-04-03T08:40:05.380709Z"
    },
    "papermill": {
     "duration": 0.012533,
     "end_time": "2025-04-03T08:40:05.382635",
     "exception": false,
     "start_time": "2025-04-03T08:40:05.370102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download_file('/kaggle/working/output_text', 'detections')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a8444",
   "metadata": {
    "papermill": {
     "duration": 0.007775,
     "end_time": "2025-04-03T08:40:05.398584",
     "exception": false,
     "start_time": "2025-04-03T08:40:05.390809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2.  Re-id (trans-ReId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb54713",
   "metadata": {
    "papermill": {
     "duration": 0.007592,
     "end_time": "2025-04-03T08:40:05.414006",
     "exception": false,
     "start_time": "2025-04-03T08:40:05.406414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Loss (metric learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a63320",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:05.430765Z",
     "iopub.status.busy": "2025-04-03T08:40:05.430537Z",
     "iopub.status.idle": "2025-04-03T08:40:06.982431Z",
     "shell.execute_reply": "2025-04-03T08:40:06.981484Z"
    },
    "papermill": {
     "duration": 1.562191,
     "end_time": "2025-04-03T08:40:06.984059",
     "exception": false,
     "start_time": "2025-04-03T08:40:05.421868",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd\n",
    "from torch.nn import Parameter\n",
    "import math\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=0.3, **kwargs):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        n = inputs.size(0)\n",
    "        # Compute similarity matrix\n",
    "        sim_mat = torch.matmul(inputs, inputs.t())\n",
    "        targets = targets\n",
    "        loss = list()\n",
    "        c = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            pos_pair_ = torch.masked_select(sim_mat[i], targets == targets[i])\n",
    "\n",
    "            #  move itself\n",
    "            pos_pair_ = torch.masked_select(pos_pair_, pos_pair_ < 1)\n",
    "            neg_pair_ = torch.masked_select(sim_mat[i], targets != targets[i])\n",
    "\n",
    "            pos_pair_ = torch.sort(pos_pair_)[0]\n",
    "            neg_pair_ = torch.sort(neg_pair_)[0]\n",
    "\n",
    "            neg_pair = torch.masked_select(neg_pair_, neg_pair_ > self.margin)\n",
    "\n",
    "            neg_loss = 0\n",
    "\n",
    "            pos_loss = torch.sum(-pos_pair_ + 1)\n",
    "            if len(neg_pair) > 0:\n",
    "                neg_loss = torch.sum(neg_pair)\n",
    "            loss.append(pos_loss + neg_loss)\n",
    "\n",
    "        loss = sum(loss) / n\n",
    "        return loss\n",
    "\n",
    "\n",
    "class CircleLoss(nn.Module):\n",
    "    def __init__(self, in_features, num_classes, s=256, m=0.25):\n",
    "        super(CircleLoss, self).__init__()\n",
    "        self.weight = Parameter(torch.Tensor(num_classes, in_features))\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self._num_classes = num_classes\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "    def __call__(self, bn_feat, targets):\n",
    "\n",
    "        sim_mat = F.linear(F.normalize(bn_feat), F.normalize(self.weight))\n",
    "        alpha_p = torch.clamp_min(-sim_mat.detach() + 1 + self.m, min=0.)\n",
    "        alpha_n = torch.clamp_min(sim_mat.detach() + self.m, min=0.)\n",
    "        delta_p = 1 - self.m\n",
    "        delta_n = self.m\n",
    "\n",
    "        s_p = self.s * alpha_p * (sim_mat - delta_p)\n",
    "        s_n = self.s * alpha_n * (sim_mat - delta_n)\n",
    "\n",
    "        targets = F.one_hot(targets, num_classes=self._num_classes)\n",
    "\n",
    "        pred_class_logits = targets * s_p + (1.0 - targets) * s_n\n",
    "\n",
    "        return pred_class_logits\n",
    "\n",
    "\n",
    "class Arcface(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.30, easy_margin=False, ls_eps=0.0):\n",
    "        super(Arcface, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = phi.type_as(cosine)\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.s\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Cosface(nn.Module):\n",
    "    r\"\"\"Implement of large margin cosine distance: :\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        s: norm of input feature\n",
    "        m: margin\n",
    "        cos(theta) - m\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.30):\n",
    "        super(Cosface, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        phi = cosine - self.m\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        # one_hot = one_hot.cuda() if cosine.is_cuda else one_hot\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' \\\n",
    "               + 'in_features=' + str(self.in_features) \\\n",
    "               + ', out_features=' + str(self.out_features) \\\n",
    "               + ', s=' + str(self.s) \\\n",
    "               + ', m=' + str(self.m) + ')'\n",
    "\n",
    "\n",
    "class AMSoftmax(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.30):\n",
    "        super(AMSoftmax, self).__init__()\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.in_feats = in_features\n",
    "        self.W = torch.nn.Parameter(torch.randn(in_features, out_features), requires_grad=True)\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        nn.init.xavier_normal_(self.W, gain=1)\n",
    "\n",
    "    def forward(self, x, lb):\n",
    "        assert x.size()[0] == lb.size()[0]\n",
    "        assert x.size()[1] == self.in_feats\n",
    "        x_norm = torch.norm(x, p=2, dim=1, keepdim=True).clamp(min=1e-12)\n",
    "        x_norm = torch.div(x, x_norm)\n",
    "        w_norm = torch.norm(self.W, p=2, dim=0, keepdim=True).clamp(min=1e-12)\n",
    "        w_norm = torch.div(self.W, w_norm)\n",
    "        costh = torch.mm(x_norm, w_norm)\n",
    "        # print(x_norm.shape, w_norm.shape, costh.shape)\n",
    "        lb_view = lb.view(-1, 1)\n",
    "        delt_costh = torch.zeros(costh.size(), device='cuda').scatter_(1, lb_view, self.m)\n",
    "        costh_m = costh - delt_costh\n",
    "        costh_m_s = self.s * costh_m\n",
    "        return costh_m_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e4e3f7",
   "metadata": {
    "papermill": {
     "duration": 0.00784,
     "end_time": "2025-04-03T08:40:07.000234",
     "exception": false,
     "start_time": "2025-04-03T08:40:06.992394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  2.2 Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767695e4",
   "metadata": {
    "papermill": {
     "duration": 0.007499,
     "end_time": "2025-04-03T08:40:07.015575",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.008076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  2.2.1 ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01ebca2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:07.032549Z",
     "iopub.status.busy": "2025-04-03T08:40:07.032202Z",
     "iopub.status.idle": "2025-04-03T08:40:07.149789Z",
     "shell.execute_reply": "2025-04-03T08:40:07.149129Z"
    },
    "papermill": {
     "duration": 0.128108,
     "end_time": "2025-04-03T08:40:07.151386",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.023278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Vision Transformer (ViT) in PyTorch\n",
    "\n",
    "A PyTorch implement of Vision Transformers as described in\n",
    "'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929\n",
    "\n",
    "The official jax code is released and available at https://github.com/google-research/vision_transformer\n",
    "\n",
    "Status/TODO:\n",
    "* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.\n",
    "* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.\n",
    "* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.\n",
    "* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.\n",
    "\n",
    "Acknowledgments:\n",
    "* The paper authors for releasing code and weights, thanks!\n",
    "* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\n",
    "for some einops/einsum fun\n",
    "* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n",
    "* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n",
    "\n",
    "Hacked together by / Copyright 2020 Ross Wightman\n",
    "\"\"\"\n",
    "import math\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch._six import container_abcs\n",
    "\n",
    "\n",
    "# From PyTorch internals\n",
    "def _ntuple(n):\n",
    "    def parse(x):\n",
    "        if isinstance(x, str): \n",
    "            return x\n",
    "        return tuple(repeat(x, n))\n",
    "    return parse\n",
    "\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "to_2tuple = _ntuple(2)\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "\n",
    "default_cfgs = {\n",
    "    # patch models\n",
    "    'vit_small_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth',\n",
    "    ),\n",
    "    'vit_base_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_224-80ecf9dd.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n",
    "    ),\n",
    "    'vit_base_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p16_384-83fb41ba.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_base_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_p32_384-830016f5.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch16_224': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_224-4ee7a4dc.pth',\n",
    "        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "    'vit_large_patch16_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p16_384-b3be5167.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_large_patch32_384': _cfg(\n",
    "        url='https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_p32_384-9b920ba8.pth',\n",
    "        input_size=(3, 384, 384), mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5), crop_pct=1.0),\n",
    "    'vit_huge_patch16_224': _cfg(),\n",
    "    'vit_huge_patch32_384': _cfg(input_size=(3, 384, 384)),\n",
    "    # hybrid models\n",
    "    'vit_small_resnet26d_224': _cfg(),\n",
    "    'vit_small_resnet50d_s3_224': _cfg(),\n",
    "    'vit_base_resnet26d_224': _cfg(),\n",
    "    'vit_base_resnet50d_224': _cfg(),\n",
    "}\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HybridEmbed(nn.Module):\n",
    "    \"\"\" CNN Feature Map Embedding\n",
    "    Extract feature map from CNN, flatten, project to embedding dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        assert isinstance(backbone, nn.Module)\n",
    "        img_size = to_2tuple(img_size)\n",
    "        self.img_size = img_size\n",
    "        self.backbone = backbone\n",
    "        if feature_size is None:\n",
    "            with torch.no_grad():\n",
    "                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n",
    "                # map for all networks, the feature metadata has reliable channel and stride info, but using\n",
    "                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n",
    "                training = backbone.training\n",
    "                if training:\n",
    "                    backbone.eval()\n",
    "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))\n",
    "                if isinstance(o, (list, tuple)):\n",
    "                    o = o[-1]  # last feature if backbone outputs list/tuple of features\n",
    "                feature_size = o.shape[-2:]\n",
    "                feature_dim = o.shape[1]\n",
    "                backbone.train(training)\n",
    "        else:\n",
    "            feature_size = to_2tuple(feature_size)\n",
    "            if hasattr(self.backbone, 'feature_info'):\n",
    "                feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "            else:\n",
    "                feature_dim = self.backbone.num_features\n",
    "        self.num_patches = feature_size[0] * feature_size[1]\n",
    "        self.proj = nn.Conv2d(feature_dim, embed_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            x = x[-1]  # last feature if backbone outputs list/tuple of features\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbed_overlap(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding with overlapping patches\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, stride_size=20, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        # img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        # stride_size_tuple = to_2tuple(stride_size)\n",
    "        self.num_x = (img_size[1] - patch_size[1]) // stride_size[1] + 1\n",
    "        self.num_y = (img_size[0] - patch_size[0]) // stride_size[0] + 1\n",
    "        print('using stride: {}, and patch number is num_y{} * num_x{}'.format(stride_size, self.num_y, self.num_x))\n",
    "        num_patches = self.num_x * self.num_y\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride_size)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.InstanceNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x)\n",
    "\n",
    "        x = x.flatten(2).transpose(1, 2) # [64, 8, 768]\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransReID(nn.Module):\n",
    "    \"\"\" Transformer-based Object Re-Identification\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, stride_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., camera=0, view=0,\n",
    "                 drop_path_rate=0., hybrid_backbone=None, norm_layer=nn.LayerNorm, local_feature=False, sie_xishu =1.0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.local_feature = local_feature\n",
    "        if hybrid_backbone is not None:\n",
    "            self.patch_embed = HybridEmbed(\n",
    "                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        else:\n",
    "            self.patch_embed = PatchEmbed_overlap(\n",
    "                img_size=img_size, patch_size=patch_size, stride_size=stride_size, in_chans=in_chans,\n",
    "                embed_dim=embed_dim)\n",
    "\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.cam_num = camera\n",
    "        self.view_num = view\n",
    "        self.sie_xishu = sie_xishu\n",
    "        # Initialize SIE Embedding\n",
    "        if camera > 1 and view > 1:\n",
    "            self.sie_embed = nn.Parameter(torch.zeros(camera * view, 1, embed_dim))\n",
    "            trunc_normal_(self.sie_embed, std=.02)\n",
    "            print('camera number is : {} and viewpoint number is : {}'.format(camera, view))\n",
    "            print('using SIE_Lambda is : {}'.format(sie_xishu))\n",
    "        elif camera > 1:\n",
    "            self.sie_embed = nn.Parameter(torch.zeros(camera, 1, embed_dim))\n",
    "            trunc_normal_(self.sie_embed, std=.02)\n",
    "            print('camera number is : {}'.format(camera))\n",
    "            print('using SIE_Lambda is : {}'.format(sie_xishu))\n",
    "        elif view > 1:\n",
    "            self.sie_embed = nn.Parameter(torch.zeros(view, 1, embed_dim))\n",
    "            trunc_normal_(self.sie_embed, std=.02)\n",
    "            print('viewpoint number is : {}'.format(view))\n",
    "            print('using SIE_Lambda is : {}'.format(sie_xishu))\n",
    "\n",
    "        print('using drop_out rate is : {}'.format(drop_rate))\n",
    "        print('using attn_drop_out rate is : {}'.format(attn_drop_rate))\n",
    "        print('using drop_path rate is : {}'.format(drop_path_rate))\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Classifier head\n",
    "        self.fc = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.fc = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x, camera_id, view_id):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        if self.cam_num > 0 and self.view_num > 0:\n",
    "            x = x + self.pos_embed + self.sie_xishu * self.sie_embed[camera_id * self.view_num + view_id]\n",
    "        elif self.cam_num > 0:\n",
    "            x = x + self.pos_embed + self.sie_xishu * self.sie_embed[camera_id]\n",
    "        elif self.view_num > 0:\n",
    "            x = x + self.pos_embed + self.sie_xishu * self.sie_embed[view_id]\n",
    "        else:\n",
    "            x = x + self.pos_embed\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        if self.local_feature:\n",
    "            for blk in self.blocks[:-1]:\n",
    "                x = blk(x)\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x)\n",
    "\n",
    "            x = self.norm(x)\n",
    "\n",
    "            return x[:, 0]\n",
    "\n",
    "    def forward(self, x, cam_label=None, view_label=None):\n",
    "        x = self.forward_features(x, cam_label, view_label)\n",
    "        return x\n",
    "\n",
    "    def load_param(self, model_path):\n",
    "        param_dict = torch.load(model_path, map_location='cpu')\n",
    "        if 'model' in param_dict:\n",
    "            param_dict = param_dict['model']\n",
    "        if 'state_dict' in param_dict:\n",
    "            param_dict = param_dict['state_dict']\n",
    "        for k, v in param_dict.items():\n",
    "            if 'head' in k or 'dist' in k:\n",
    "                continue\n",
    "            if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "                # For old models that I trained prior to conv based patchification\n",
    "                O, I, H, W = self.patch_embed.proj.weight.shape\n",
    "                v = v.reshape(O, -1, H, W)\n",
    "            elif k == 'pos_embed' and v.shape != self.pos_embed.shape:\n",
    "                # To resize pos embedding when using model at different size from pretrained weights\n",
    "                if 'distilled' in model_path:\n",
    "                    print('distill need to choose right cls token in the pth')\n",
    "                    v = torch.cat([v[:, 0:1], v[:, 2:]], dim=1)\n",
    "                v = resize_pos_embed(v, self.pos_embed, self.patch_embed.num_y, self.patch_embed.num_x)\n",
    "            try:\n",
    "                self.state_dict()[k].copy_(v)\n",
    "            except:\n",
    "                print('===========================ERROR=========================')\n",
    "                print('shape do not match in k :{}: param_dict{} vs self.state_dict(){}'.format(k, v.shape, self.state_dict()[k].shape))\n",
    "\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new, hight, width):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "\n",
    "    posemb_token, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "    ntok_new -= 1\n",
    "\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    print('Resized position embedding from size:{} to size: {} with height:{} width: {}'.format(posemb.shape, posemb_new.shape, hight, width))\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(hight, width), mode='bilinear')\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, hight * width, -1)\n",
    "    posemb = torch.cat([posemb_token, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def vit_base_patch16_224_TransReID(img_size=(256, 128), stride_size=16, drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.1, camera=0, view=0,local_feature=False,sie_xishu=1.5, **kwargs):\n",
    "    model = TransReID(in_chans=3,\n",
    "        img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\\\n",
    "        camera=camera, view=view, drop_path_rate=drop_path_rate, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),  sie_xishu=sie_xishu, local_feature=local_feature, **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def vit_small_patch16_224_TransReID(img_size=(256, 128), stride_size=16, drop_rate=0., attn_drop_rate=0.,drop_path_rate=0.1, camera=0, view=0, local_feature=False, sie_xishu=1.5, **kwargs):\n",
    "    kwargs.setdefault('qk_scale', 768 ** -0.5)\n",
    "    model = TransReID(\n",
    "        img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=768, depth=8, num_heads=8,  mlp_ratio=3., qkv_bias=False, drop_path_rate = drop_path_rate,\\\n",
    "        camera=camera, view=view,  drop_rate=drop_rate, attn_drop_rate=attn_drop_rate,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6),  sie_xishu=sie_xishu, local_feature=local_feature, **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def deit_small_patch16_224_TransReID(img_size=(256, 128), stride_size=16, drop_path_rate=0.1, drop_rate=0.0, attn_drop_rate=0.0, camera=0, view=0, local_feature=False, sie_xishu=1.5, **kwargs):\n",
    "    model = TransReID(\n",
    "        img_size=img_size, patch_size=16, stride_size=stride_size, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n",
    "        drop_path_rate=drop_path_rate, drop_rate=drop_rate, attn_drop_rate=attn_drop_rate, camera=camera, view=view, sie_xishu=sie_xishu, local_feature=local_feature,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        print(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9a21b",
   "metadata": {
    "papermill": {
     "duration": 0.007532,
     "end_time": "2025-04-03T08:40:07.167098",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.159566",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  2.2.2 ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff5982b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:07.183798Z",
     "iopub.status.busy": "2025-04-03T08:40:07.183576Z",
     "iopub.status.idle": "2025-04-03T08:40:07.197717Z",
     "shell.execute_reply": "2025-04-03T08:40:07.197171Z"
    },
    "papermill": {
     "duration": 0.023699,
     "end_time": "2025-04-03T08:40:07.198804",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.175105",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, last_stride=2, block=Bottleneck,layers=[3, 4, 6, 3]):\n",
    "        self.inplanes = 64\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # self.relu = nn.ReLU(inplace=True)   # add missed relu\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=None, padding=0)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=last_stride)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cam_label=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        # x = self.relu(x)    # add missed relu\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def load_param(self, model_path):\n",
    "        param_dict = torch.load(model_path)\n",
    "        for i in param_dict:\n",
    "            if 'fc' in i:\n",
    "                continue\n",
    "            self.state_dict()[i].copy_(param_dict[i])\n",
    "\n",
    "    def random_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f6c40",
   "metadata": {
    "papermill": {
     "duration": 0.007654,
     "end_time": "2025-04-03T08:40:07.214205",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.206551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.3 Build TransNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "559c9060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:07.230961Z",
     "iopub.status.busy": "2025-04-03T08:40:07.230729Z",
     "iopub.status.idle": "2025-04-03T08:40:07.267801Z",
     "shell.execute_reply": "2025-04-03T08:40:07.267042Z"
    },
    "papermill": {
     "duration": 0.04706,
     "end_time": "2025-04-03T08:40:07.269063",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.222003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from .backbones.resnet import ResNet, Bottleneck\n",
    "import copy\n",
    "# from .backbones.vit_pytorch import vit_base_patch16_224_TransReID, vit_small_patch16_224_TransReID, deit_small_patch16_224_TransReID\n",
    "# from reidmodel.loss.metric_learning import Arcface, Cosface, AMSoftmax, CircleLoss\n",
    "\n",
    "def shuffle_unit(features, shift, group, begin=1):\n",
    "\n",
    "    batchsize = features.size(0)\n",
    "    dim = features.size(-1)\n",
    "    # Shift Operation\n",
    "    feature_random = torch.cat([features[:, begin-1+shift:], features[:, begin:begin-1+shift]], dim=1)\n",
    "    x = feature_random\n",
    "    # Patch Shuffle Operation\n",
    "    try:\n",
    "        x = x.view(batchsize, group, -1, dim)\n",
    "    except:\n",
    "        x = torch.cat([x, x[:, -2:-1, :]], dim=1)\n",
    "        x = x.view(batchsize, group, -1, dim)\n",
    "\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "    x = x.view(batchsize, -1, dim)\n",
    "\n",
    "    return x\n",
    "\n",
    "def weights_init_kaiming(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    elif classname.find('Conv') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        if m.affine:\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "def weights_init_classifier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight, std=0.001)\n",
    "        if m.bias:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, num_classes, cfg):\n",
    "        super(Backbone, self).__init__()\n",
    "        last_stride = cfg.MODEL.LAST_STRIDE\n",
    "        model_path = cfg.MODEL.PRETRAIN_PATH\n",
    "        model_name = cfg.MODEL.NAME\n",
    "        pretrain_choice = cfg.MODEL.PRETRAIN_CHOICE\n",
    "        self.cos_layer = cfg.MODEL.COS_LAYER\n",
    "        self.neck = cfg.MODEL.NECK\n",
    "        self.neck_feat = cfg.TEST.NECK_FEAT\n",
    "\n",
    "        if model_name == 'resnet50':\n",
    "            self.in_planes = 2048\n",
    "            self.base = ResNet(last_stride=last_stride,\n",
    "                               block=Bottleneck,\n",
    "                               layers=[3, 4, 6, 3])\n",
    "            print('using resnet50 as a backbone')\n",
    "        else:\n",
    "            print('unsupported backbone! but got {}'.format(model_name))\n",
    "\n",
    "        if pretrain_choice == 'imagenet':\n",
    "            self.base.load_param(model_path)\n",
    "            print('Loading pretrained ImageNet model......from {}'.format(model_path))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)\n",
    "        self.classifier.apply(weights_init_classifier)\n",
    "\n",
    "        self.bottleneck = nn.BatchNorm1d(self.in_planes)\n",
    "        self.bottleneck.bias.requires_grad_(False)\n",
    "        self.bottleneck.apply(weights_init_kaiming)\n",
    "\n",
    "    def forward(self, x, label=None):  # label is unused if self.cos_layer == 'no'\n",
    "        x = self.base(x)\n",
    "        global_feat = nn.functional.avg_pool2d(x, x.shape[2:4])\n",
    "        global_feat = global_feat.view(global_feat.shape[0], -1)  # flatten to (bs, 2048)\n",
    "\n",
    "        if self.neck == 'no':\n",
    "            feat = global_feat\n",
    "        elif self.neck == 'bnneck':\n",
    "            feat = self.bottleneck(global_feat)\n",
    "\n",
    "        if self.training:\n",
    "            if self.cos_layer:\n",
    "                cls_score = self.arcface(feat, label)\n",
    "            else:\n",
    "                cls_score = self.classifier(feat)\n",
    "            return cls_score, global_feat\n",
    "        else:\n",
    "            if self.neck_feat == 'after':\n",
    "                return feat\n",
    "            else:\n",
    "                return global_feat\n",
    "\n",
    "    def load_param(self, trained_path):\n",
    "        param_dict = torch.load(trained_path)\n",
    "        if 'state_dict' in param_dict:\n",
    "            param_dict = param_dict['state_dict']\n",
    "        for i in param_dict:\n",
    "            self.state_dict()[i].copy_(param_dict[i])\n",
    "        print('Loading pretrained model from {}'.format(trained_path))\n",
    "\n",
    "    def load_param_finetune(self, model_path):\n",
    "        param_dict = torch.load(model_path)\n",
    "        for i in param_dict:\n",
    "            self.state_dict()[i].copy_(param_dict[i])\n",
    "        print('Loading pretrained model for finetuning from {}'.format(model_path))\n",
    "\n",
    "\n",
    "class build_transformer(nn.Module):\n",
    "    def __init__(self, num_classes, camera_num, view_num, cfg, factory):\n",
    "        super(build_transformer, self).__init__()\n",
    "        last_stride = cfg.MODEL.LAST_STRIDE\n",
    "        model_path = cfg.MODEL.PRETRAIN_PATH\n",
    "        model_name = cfg.MODEL.NAME\n",
    "        pretrain_choice = cfg.MODEL.PRETRAIN_CHOICE\n",
    "        self.cos_layer = cfg.MODEL.COS_LAYER\n",
    "        self.neck = cfg.MODEL.NECK\n",
    "        self.neck_feat = cfg.TEST.NECK_FEAT\n",
    "        self.in_planes = 768\n",
    "\n",
    "        print('using Transformer_type: {} as a backbone'.format(cfg.MODEL.TRANSFORMER_TYPE))\n",
    "\n",
    "        if cfg.MODEL.SIE_CAMERA:\n",
    "            camera_num = camera_num\n",
    "        else:\n",
    "            camera_num = 0\n",
    "        if cfg.MODEL.SIE_VIEW:\n",
    "            view_num = view_num\n",
    "        else:\n",
    "            view_num = 0\n",
    "\n",
    "        self.base = factory[cfg.MODEL.TRANSFORMER_TYPE](img_size=cfg.INPUT.SIZE_TRAIN, sie_xishu=cfg.MODEL.SIE_COE,\n",
    "                                                        camera=camera_num, view=view_num, stride_size=cfg.MODEL.STRIDE_SIZE, drop_path_rate=cfg.MODEL.DROP_PATH,\n",
    "                                                        drop_rate= cfg.MODEL.DROP_OUT,\n",
    "                                                        attn_drop_rate=cfg.MODEL.ATT_DROP_RATE)\n",
    "        if cfg.MODEL.TRANSFORMER_TYPE == 'deit_small_patch16_224_TransReID':\n",
    "            self.in_planes = 384\n",
    "        if pretrain_choice == 'imagenet':\n",
    "            self.base.load_param(model_path)\n",
    "            print('Loading pretrained ImageNet model......from {}'.format(model_path))\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.ID_LOSS_TYPE = cfg.MODEL.ID_LOSS_TYPE\n",
    "        if self.ID_LOSS_TYPE == 'arcface':\n",
    "            print('using {} with s:{}, m: {}'.format(self.ID_LOSS_TYPE,cfg.SOLVER.COSINE_SCALE,cfg.SOLVER.COSINE_MARGIN))\n",
    "            self.classifier = Arcface(self.in_planes, self.num_classes,\n",
    "                                      s=cfg.SOLVER.COSINE_SCALE, m=cfg.SOLVER.COSINE_MARGIN)\n",
    "        elif self.ID_LOSS_TYPE == 'cosface':\n",
    "            print('using {} with s:{}, m: {}'.format(self.ID_LOSS_TYPE,cfg.SOLVER.COSINE_SCALE,cfg.SOLVER.COSINE_MARGIN))\n",
    "            self.classifier = Cosface(self.in_planes, self.num_classes,\n",
    "                                      s=cfg.SOLVER.COSINE_SCALE, m=cfg.SOLVER.COSINE_MARGIN)\n",
    "        elif self.ID_LOSS_TYPE == 'amsoftmax':\n",
    "            print('using {} with s:{}, m: {}'.format(self.ID_LOSS_TYPE,cfg.SOLVER.COSINE_SCALE,cfg.SOLVER.COSINE_MARGIN))\n",
    "            self.classifier = AMSoftmax(self.in_planes, self.num_classes,\n",
    "                                        s=cfg.SOLVER.COSINE_SCALE, m=cfg.SOLVER.COSINE_MARGIN)\n",
    "        elif self.ID_LOSS_TYPE == 'circle':\n",
    "            print('using {} with s:{}, m: {}'.format(self.ID_LOSS_TYPE, cfg.SOLVER.COSINE_SCALE, cfg.SOLVER.COSINE_MARGIN))\n",
    "            self.classifier = CircleLoss(self.in_planes, self.num_classes,\n",
    "                                        s=cfg.SOLVER.COSINE_SCALE, m=cfg.SOLVER.COSINE_MARGIN)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)\n",
    "            self.classifier.apply(weights_init_classifier)\n",
    "\n",
    "        self.bottleneck = nn.BatchNorm1d(self.in_planes)\n",
    "        self.bottleneck.bias.requires_grad_(False)\n",
    "        self.bottleneck.apply(weights_init_kaiming)\n",
    "\n",
    "    def forward(self, x, label=None, cam_label= None, view_label=None):\n",
    "        global_feat = self.base(x, cam_label=cam_label, view_label=view_label)\n",
    "\n",
    "        feat = self.bottleneck(global_feat)\n",
    "\n",
    "        if self.training:\n",
    "            if self.ID_LOSS_TYPE in ('arcface', 'cosface', 'amsoftmax', 'circle'):\n",
    "                cls_score = self.classifier(feat, label)\n",
    "            else:\n",
    "                cls_score = self.classifier(feat)\n",
    "\n",
    "            return cls_score, global_feat  # global feature for triplet loss\n",
    "        else:\n",
    "            if self.neck_feat == 'after':\n",
    "                # print(\"Test with feature after BN\")\n",
    "                return feat\n",
    "            else:\n",
    "                # print(\"Test with feature before BN\")\n",
    "                return global_feat\n",
    "\n",
    "    def load_param(self, trained_path):\n",
    "        param_dict = torch.load(trained_path)\n",
    "        for i in param_dict:\n",
    "            self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])\n",
    "        print('Loading pretrained model from {}'.format(trained_path))\n",
    "\n",
    "    def load_param_finetune(self, model_path):\n",
    "        param_dict = torch.load(model_path)\n",
    "        for i in param_dict:\n",
    "            self.state_dict()[i].copy_(param_dict[i])\n",
    "        print('Loading pretrained model for finetuning from {}'.format(model_path))\n",
    "\n",
    "\n",
    "class build_transformer_local(nn.Module):\n",
    "    def __init__(self, num_classes, camera_num, view_num, cfg, factory, rearrange):\n",
    "        super(build_transformer_local, self).__init__()\n",
    "        model_path = cfg.MODEL.PRETRAIN_PATH\n",
    "        pretrain_choice = cfg.MODEL.PRETRAIN_CHOICE\n",
    "        self.cos_layer = cfg.MODEL.COS_LAYER\n",
    "        self.neck = cfg.MODEL.NECK\n",
    "        self.neck_feat = cfg.TEST.NECK_FEAT\n",
    "        self.in_planes = 768\n",
    "\n",
    "        print('using Transformer_type: {} as a backbone'.format(cfg.MODEL.TRANSFORMER_TYPE))\n",
    "\n",
    "        if cfg.MODEL.SIE_CAMERA:\n",
    "            camera_num = camera_num\n",
    "        else:\n",
    "            camera_num = 0\n",
    "\n",
    "        if cfg.MODEL.SIE_VIEW:\n",
    "            view_num = view_num\n",
    "        else:\n",
    "            view_num = 0\n",
    "\n",
    "        self.base = factory[cfg.MODEL.TRANSFORMER_TYPE](img_size=cfg.INPUT.SIZE_TRAIN, sie_xishu=cfg.MODEL.SIE_COE, local_feature=cfg.MODEL.JPM, camera=camera_num, view=view_num, stride_size=cfg.MODEL.STRIDE_SIZE, drop_path_rate=cfg.MODEL.DROP_PATH)\n",
    "\n",
    "        if pretrain_choice == 'imagenet':\n",
    "            self.base.load_param(model_path)\n",
    "            print('Loading pretrained ImageNet model......from {}'.format(model_path))\n",
    "\n",
    "        block = self.base.blocks[-1]\n",
    "        layer_norm = self.base.norm\n",
    "        self.b1 = nn.Sequential(\n",
    "            copy.deepcopy(block),\n",
    "            copy.deepcopy(layer_norm)\n",
    "        )\n",
    "        self.b2 = nn.Sequential(\n",
    "            copy.deepcopy(block),\n",
    "            copy.deepcopy(layer_norm)\n",
    "        )\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.ID_LOSS_TYPE = cfg.MODEL.ID_LOSS_TYPE\n",
    "        if self.ID_LOSS_TYPE == 'arcface':\n",
    "            print('using {} with s:{}, m: {}'.format(self.ID_LOSS_TYPE,cfg.SOLVER.COSINE_SCALE,cfg.SOLVER.COSINE_MARGIN))\n",
    "            self.classifier = Arcface(self.in_planes, self.num_classes,\n",
    "                                      s=cfg.SOLVER.COSINE_SCALE, m=cfg.SOLVER.COSINE_MARGIN)\n",
    "        elif self.ID_LOSS_TYPE == 'cosface':\n",
    "            print('using {} with s:{}, m: {}'.format(self.ID_LOSS_TYPE,cfg.SOLVER.COSINE_SCALE,cfg.SOLVER.COSINE_MARGIN))\n",
    "            self.classifier = Cosface(self.in_planes, self.num_classes,\n",
    "                                      s=cfg.SOLVER.COSINE_SCALE, m=cfg.SOLVER.COSINE_MARGIN)\n",
    "        elif self.ID_LOSS_TYPE == 'amsoftmax':\n",
    "            print('using {} with s:{}, m: {}'.format(self.ID_LOSS_TYPE,cfg.SOLVER.COSINE_SCALE,cfg.SOLVER.COSINE_MARGIN))\n",
    "            self.classifier = AMSoftmax(self.in_planes, self.num_classes,\n",
    "                                        s=cfg.SOLVER.COSINE_SCALE, m=cfg.SOLVER.COSINE_MARGIN)\n",
    "        elif self.ID_LOSS_TYPE == 'circle':\n",
    "            print('using {} with s:{}, m: {}'.format(self.ID_LOSS_TYPE, cfg.SOLVER.COSINE_SCALE, cfg.SOLVER.COSINE_MARGIN))\n",
    "            self.classifier = CircleLoss(self.in_planes, self.num_classes,\n",
    "                                        s=cfg.SOLVER.COSINE_SCALE, m=cfg.SOLVER.COSINE_MARGIN)\n",
    "        else:\n",
    "            self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)\n",
    "            self.classifier.apply(weights_init_classifier)\n",
    "            self.classifier_1 = nn.Linear(self.in_planes, self.num_classes, bias=False)\n",
    "            self.classifier_1.apply(weights_init_classifier)\n",
    "            self.classifier_2 = nn.Linear(self.in_planes, self.num_classes, bias=False)\n",
    "            self.classifier_2.apply(weights_init_classifier)\n",
    "            self.classifier_3 = nn.Linear(self.in_planes, self.num_classes, bias=False)\n",
    "            self.classifier_3.apply(weights_init_classifier)\n",
    "            self.classifier_4 = nn.Linear(self.in_planes, self.num_classes, bias=False)\n",
    "            self.classifier_4.apply(weights_init_classifier)\n",
    "\n",
    "        self.bottleneck = nn.BatchNorm1d(self.in_planes)\n",
    "        self.bottleneck.bias.requires_grad_(False)\n",
    "        self.bottleneck.apply(weights_init_kaiming)\n",
    "        self.bottleneck_1 = nn.BatchNorm1d(self.in_planes)\n",
    "        self.bottleneck_1.bias.requires_grad_(False)\n",
    "        self.bottleneck_1.apply(weights_init_kaiming)\n",
    "        self.bottleneck_2 = nn.BatchNorm1d(self.in_planes)\n",
    "        self.bottleneck_2.bias.requires_grad_(False)\n",
    "        self.bottleneck_2.apply(weights_init_kaiming)\n",
    "        self.bottleneck_3 = nn.BatchNorm1d(self.in_planes)\n",
    "        self.bottleneck_3.bias.requires_grad_(False)\n",
    "        self.bottleneck_3.apply(weights_init_kaiming)\n",
    "        self.bottleneck_4 = nn.BatchNorm1d(self.in_planes)\n",
    "        self.bottleneck_4.bias.requires_grad_(False)\n",
    "        self.bottleneck_4.apply(weights_init_kaiming)\n",
    "\n",
    "        self.shuffle_groups = cfg.MODEL.SHUFFLE_GROUP\n",
    "        print('using shuffle_groups size:{}'.format(self.shuffle_groups))\n",
    "        self.shift_num = cfg.MODEL.SHIFT_NUM\n",
    "        print('using shift_num size:{}'.format(self.shift_num))\n",
    "        self.divide_length = cfg.MODEL.DEVIDE_LENGTH\n",
    "        print('using divide_length size:{}'.format(self.divide_length))\n",
    "        self.rearrange = rearrange\n",
    "\n",
    "    def forward(self, x, label=None, cam_label= None, view_label=None):  # label is unused if self.cos_layer == 'no'\n",
    "\n",
    "        features = self.base(x, cam_label=cam_label, view_label=view_label)\n",
    "\n",
    "        # global branch\n",
    "        b1_feat = self.b1(features) # [64, 129, 768]\n",
    "        global_feat = b1_feat[:, 0]\n",
    "\n",
    "        # JPM branch\n",
    "        feature_length = features.size(1) - 1\n",
    "        patch_length = feature_length // self.divide_length\n",
    "        token = features[:, 0:1]\n",
    "\n",
    "        if self.rearrange:\n",
    "            x = shuffle_unit(features, self.shift_num, self.shuffle_groups)\n",
    "        else:\n",
    "            x = features[:, 1:]\n",
    "        # lf_1\n",
    "        b1_local_feat = x[:, :patch_length]\n",
    "        b1_local_feat = self.b2(torch.cat((token, b1_local_feat), dim=1))\n",
    "        local_feat_1 = b1_local_feat[:, 0]\n",
    "\n",
    "        # lf_2\n",
    "        b2_local_feat = x[:, patch_length:patch_length*2]\n",
    "        b2_local_feat = self.b2(torch.cat((token, b2_local_feat), dim=1))\n",
    "        local_feat_2 = b2_local_feat[:, 0]\n",
    "\n",
    "        # lf_3\n",
    "        b3_local_feat = x[:, patch_length*2:patch_length*3]\n",
    "        b3_local_feat = self.b2(torch.cat((token, b3_local_feat), dim=1))\n",
    "        local_feat_3 = b3_local_feat[:, 0]\n",
    "\n",
    "        # lf_4\n",
    "        b4_local_feat = x[:, patch_length*3:patch_length*4]\n",
    "        b4_local_feat = self.b2(torch.cat((token, b4_local_feat), dim=1))\n",
    "        local_feat_4 = b4_local_feat[:, 0]\n",
    "\n",
    "        feat = self.bottleneck(global_feat)\n",
    "\n",
    "        local_feat_1_bn = self.bottleneck_1(local_feat_1)\n",
    "        local_feat_2_bn = self.bottleneck_2(local_feat_2)\n",
    "        local_feat_3_bn = self.bottleneck_3(local_feat_3)\n",
    "        local_feat_4_bn = self.bottleneck_4(local_feat_4)\n",
    "\n",
    "        if self.training:\n",
    "            if self.ID_LOSS_TYPE in ('arcface', 'cosface', 'amsoftmax', 'circle'):\n",
    "                cls_score = self.classifier(feat, label)\n",
    "            else:\n",
    "                cls_score = self.classifier(feat)\n",
    "                cls_score_1 = self.classifier_1(local_feat_1_bn)\n",
    "                cls_score_2 = self.classifier_2(local_feat_2_bn)\n",
    "                cls_score_3 = self.classifier_3(local_feat_3_bn)\n",
    "                cls_score_4 = self.classifier_4(local_feat_4_bn)\n",
    "            return [cls_score, cls_score_1, cls_score_2, cls_score_3,\n",
    "                        cls_score_4\n",
    "                        ], [global_feat, local_feat_1, local_feat_2, local_feat_3,\n",
    "                            local_feat_4]  # global feature for triplet loss\n",
    "        else:\n",
    "            if self.neck_feat == 'after':\n",
    "                return torch.cat(\n",
    "                    [feat, local_feat_1_bn / 4, local_feat_2_bn / 4, local_feat_3_bn / 4, local_feat_4_bn / 4], dim=1)\n",
    "            else:\n",
    "                return torch.cat(\n",
    "                    [global_feat, local_feat_1 / 4, local_feat_2 / 4, local_feat_3 / 4, local_feat_4 / 4], dim=1)\n",
    "\n",
    "    def load_param(self, trained_path):\n",
    "        param_dict = torch.load(trained_path)\n",
    "        for i in param_dict:\n",
    "            # print(i, self.state_dict()[i.replace('module.', '')].shape, param_dict[i].shape)\n",
    "            self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])\n",
    "        print('Loading pretrained model from {}'.format(trained_path))\n",
    "\n",
    "    def load_param_finetune(self, model_path):\n",
    "        param_dict = torch.load(model_path)\n",
    "        for i in param_dict:\n",
    "            self.state_dict()[i].copy_(param_dict[i])\n",
    "        print('Loading pretrained model for finetuning from {}'.format(model_path))\n",
    "\n",
    "\n",
    "__factory_T_type = {\n",
    "    'vit_base_patch16_224_TransReID': vit_base_patch16_224_TransReID,\n",
    "    'deit_base_patch16_224_TransReID': vit_base_patch16_224_TransReID,\n",
    "    'vit_small_patch16_224_TransReID': vit_small_patch16_224_TransReID,\n",
    "    'deit_small_patch16_224_TransReID': deit_small_patch16_224_TransReID\n",
    "}\n",
    "\n",
    "def make_model(cfg, num_class, camera_num, view_num):\n",
    "    if cfg.MODEL.NAME == 'transformer':\n",
    "        if cfg.MODEL.JPM:\n",
    "            model = build_transformer_local(num_class, camera_num, view_num, cfg, __factory_T_type, rearrange=cfg.MODEL.RE_ARRANGE)\n",
    "            print('===========building transformer with JPM module ===========')\n",
    "        else:\n",
    "            model = build_transformer(num_class, camera_num, view_num, cfg, __factory_T_type)\n",
    "            print('===========building transformer===========')\n",
    "    else:\n",
    "        model = Backbone(num_class, cfg)\n",
    "        print('===========building ResNet===========')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14f199",
   "metadata": {
    "papermill": {
     "duration": 0.007511,
     "end_time": "2025-04-03T08:40:07.284488",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.276977",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.4 Re-Ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37f0bd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:07.300674Z",
     "iopub.status.busy": "2025-04-03T08:40:07.300478Z",
     "iopub.status.idle": "2025-04-03T08:40:07.311116Z",
     "shell.execute_reply": "2025-04-03T08:40:07.310520Z"
    },
    "papermill": {
     "duration": 0.020132,
     "end_time": "2025-04-03T08:40:07.312328",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.292196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri, 25 May 2018 20:29:09\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "CVPR2017 paper:Zhong Z, Zheng L, Cao D, et al. Re-ranking Person Re-identification with k-reciprocal Encoding[J]. 2017.\n",
    "url:http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf\n",
    "Matlab version: https://github.com/zhunzhong07/person-re-ranking\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "API\n",
    "\n",
    "probFea: all feature vectors of the query set (torch tensor)\n",
    "probFea: all feature vectors of the gallery set (torch tensor)\n",
    "k1,k2,lambda: parameters, the original paper is (k1=20,k2=6,lambda=0.3)\n",
    "MemorySave: set to 'True' when using MemorySave mode\n",
    "Minibatch: avaliable when 'MemorySave' is 'True'\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def re_ranking(probFea, galFea, k1, k2, lambda_value, local_distmat=None, only_local=False):\n",
    "    # if feature vector is numpy, you should use 'torch.tensor' transform it to tensor\n",
    "    query_num = probFea.size(0)\n",
    "    all_num = query_num + galFea.size(0)\n",
    "    if only_local:\n",
    "        original_dist = local_distmat\n",
    "    else:\n",
    "        feat = torch.cat([probFea, galFea])\n",
    "        # print('using GPU to compute original distance')\n",
    "        distmat = torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num) + \\\n",
    "                  torch.pow(feat, 2).sum(dim=1, keepdim=True).expand(all_num, all_num).t()\n",
    "        distmat.addmm_(1, -2, feat, feat.t())\n",
    "        original_dist = distmat.cpu().numpy()\n",
    "        del feat\n",
    "        if not local_distmat is None:\n",
    "            original_dist = original_dist + local_distmat\n",
    "    gallery_num = original_dist.shape[0]\n",
    "    original_dist = np.transpose(original_dist / np.max(original_dist, axis=0))\n",
    "    V = np.zeros_like(original_dist).astype(np.float16)\n",
    "    initial_rank = np.argsort(original_dist).astype(np.int32)\n",
    "\n",
    "    # print('starting re_ranking')\n",
    "    for i in range(all_num):\n",
    "        # k-reciprocal neighbors\n",
    "        forward_k_neigh_index = initial_rank[i, :k1 + 1]\n",
    "        backward_k_neigh_index = initial_rank[forward_k_neigh_index, :k1 + 1]\n",
    "        fi = np.where(backward_k_neigh_index == i)[0]\n",
    "        k_reciprocal_index = forward_k_neigh_index[fi]\n",
    "        k_reciprocal_expansion_index = k_reciprocal_index\n",
    "        for j in range(len(k_reciprocal_index)):\n",
    "            candidate = k_reciprocal_index[j]\n",
    "            candidate_forward_k_neigh_index = initial_rank[candidate, :int(np.around(k1 / 2)) + 1]\n",
    "            candidate_backward_k_neigh_index = initial_rank[candidate_forward_k_neigh_index,\n",
    "                                               :int(np.around(k1 / 2)) + 1]\n",
    "            fi_candidate = np.where(candidate_backward_k_neigh_index == candidate)[0]\n",
    "            candidate_k_reciprocal_index = candidate_forward_k_neigh_index[fi_candidate]\n",
    "            if len(np.intersect1d(candidate_k_reciprocal_index, k_reciprocal_index)) > 2 / 3 * len(\n",
    "                    candidate_k_reciprocal_index):\n",
    "                k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index, candidate_k_reciprocal_index)\n",
    "\n",
    "        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\n",
    "        weight = np.exp(-original_dist[i, k_reciprocal_expansion_index])\n",
    "        V[i, k_reciprocal_expansion_index] = weight / np.sum(weight)\n",
    "    original_dist = original_dist[:query_num, ]\n",
    "    if k2 != 1:\n",
    "        V_qe = np.zeros_like(V, dtype=np.float16)\n",
    "        for i in range(all_num):\n",
    "            V_qe[i, :] = np.mean(V[initial_rank[i, :k2], :], axis=0)\n",
    "        V = V_qe\n",
    "        del V_qe\n",
    "    del initial_rank\n",
    "    invIndex = []\n",
    "    for i in range(gallery_num):\n",
    "        invIndex.append(np.where(V[:, i] != 0)[0])\n",
    "\n",
    "    jaccard_dist = np.zeros_like(original_dist, dtype=np.float16)\n",
    "\n",
    "    for i in range(query_num):\n",
    "        temp_min = np.zeros(shape=[1, gallery_num], dtype=np.float16)\n",
    "        indNonZero = np.where(V[i, :] != 0)[0]\n",
    "        indImages = [invIndex[ind] for ind in indNonZero]\n",
    "        for j in range(len(indNonZero)):\n",
    "            temp_min[0, indImages[j]] = temp_min[0, indImages[j]] + np.minimum(V[i, indNonZero[j]],\n",
    "                                                                               V[indImages[j], indNonZero[j]])\n",
    "        jaccard_dist[i] = 1 - temp_min / (2 - temp_min)\n",
    "\n",
    "    final_dist = jaccard_dist * (1 - lambda_value) + original_dist * lambda_value\n",
    "    del original_dist\n",
    "    del V\n",
    "    del jaccard_dist\n",
    "    final_dist = final_dist[:query_num, query_num:]\n",
    "    return final_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0231dc59",
   "metadata": {
    "papermill": {
     "duration": 0.007611,
     "end_time": "2025-04-03T08:40:07.327560",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.319949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.5 Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3dddcc9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:07.343964Z",
     "iopub.status.busy": "2025-04-03T08:40:07.343764Z",
     "iopub.status.idle": "2025-04-03T08:40:07.356294Z",
     "shell.execute_reply": "2025-04-03T08:40:07.355703Z"
    },
    "papermill": {
     "duration": 0.021944,
     "end_time": "2025-04-03T08:40:07.357369",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.335425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "# from utils.reranking import re_ranking\n",
    "\n",
    "\n",
    "def euclidean_distance(qf, gf):\n",
    "    m = qf.shape[0]\n",
    "    n = gf.shape[0]\n",
    "    dist_mat = torch.pow(qf, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "               torch.pow(gf, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "    dist_mat.addmm_(1, -2, qf, gf.t())\n",
    "    return dist_mat.cpu().numpy()\n",
    "\n",
    "def cosine_similarity(qf, gf):\n",
    "    epsilon = 0.00001\n",
    "    dist_mat = qf.mm(gf.t())\n",
    "    qf_norm = torch.norm(qf, p=2, dim=1, keepdim=True)  # mx1\n",
    "    gf_norm = torch.norm(gf, p=2, dim=1, keepdim=True)  # nx1\n",
    "    qg_normdot = qf_norm.mm(gf_norm.t())\n",
    "\n",
    "    dist_mat = dist_mat.mul(1 / qg_normdot).cpu().numpy()\n",
    "    dist_mat = np.clip(dist_mat, -1 + epsilon, 1 - epsilon)\n",
    "    dist_mat = np.arccos(dist_mat)\n",
    "    return dist_mat\n",
    "\n",
    "\n",
    "def eval_func(distmat, q_pids, g_pids, q_camids, g_camids, max_rank=50):\n",
    "    \"\"\"Evaluation with market1501 metric\n",
    "        Key: for each query identity, its gallery images from the same camera view are discarded.\n",
    "        \"\"\"\n",
    "    num_q, num_g = distmat.shape\n",
    "    # distmat g\n",
    "    #    q    1 3 2 4\n",
    "    #         4 1 2 3\n",
    "    if num_g < max_rank:\n",
    "        max_rank = num_g\n",
    "        print(\"Note: number of gallery samples is quite small, got {}\".format(num_g))\n",
    "    indices = np.argsort(distmat, axis=1)\n",
    "    #  0 2 1 3\n",
    "    #  1 2 3 0\n",
    "    matches = (g_pids[indices] == q_pids[:, np.newaxis]).astype(np.int32)\n",
    "    # compute cmc curve for each query\n",
    "    all_cmc = []\n",
    "    all_AP = []\n",
    "    num_valid_q = 0.  # number of valid query\n",
    "    for q_idx in range(num_q):\n",
    "        # get query pid and camid\n",
    "        q_pid = q_pids[q_idx]\n",
    "        q_camid = q_camids[q_idx]\n",
    "\n",
    "        # remove gallery samples that have the same pid and camid with query\n",
    "        order = indices[q_idx]  # select one row\n",
    "        remove = (g_pids[order] == q_pid) & (g_camids[order] == q_camid)\n",
    "        keep = np.invert(remove)\n",
    "\n",
    "        # compute cmc curve\n",
    "        # binary vector, positions with value 1 are correct matches\n",
    "        orig_cmc = matches[q_idx][keep]\n",
    "        if not np.any(orig_cmc):\n",
    "            # this condition is true when query identity does not appear in gallery\n",
    "            continue\n",
    "\n",
    "        cmc = orig_cmc.cumsum()\n",
    "        cmc[cmc > 1] = 1\n",
    "\n",
    "        all_cmc.append(cmc[:max_rank])\n",
    "        num_valid_q += 1.\n",
    "\n",
    "        # compute average precision\n",
    "        # reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision\n",
    "        num_rel = orig_cmc.sum()\n",
    "        tmp_cmc = orig_cmc.cumsum()\n",
    "        #tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]\n",
    "        y = np.arange(1, tmp_cmc.shape[0] + 1) * 1.0\n",
    "        tmp_cmc = tmp_cmc / y\n",
    "        tmp_cmc = np.asarray(tmp_cmc) * orig_cmc\n",
    "        AP = tmp_cmc.sum() / num_rel\n",
    "        all_AP.append(AP)\n",
    "\n",
    "    assert num_valid_q > 0, \"Error: all query identities do not appear in gallery\"\n",
    "\n",
    "    all_cmc = np.asarray(all_cmc).astype(np.float32)\n",
    "    all_cmc = all_cmc.sum(0) / num_valid_q\n",
    "    mAP = np.mean(all_AP)\n",
    "\n",
    "    return all_cmc, mAP\n",
    "\n",
    "\n",
    "class R1_mAP_eval():\n",
    "    def __init__(self, num_query, max_rank=50, feat_norm=True, reranking=False):\n",
    "        super(R1_mAP_eval, self).__init__()\n",
    "        self.num_query = num_query\n",
    "        self.max_rank = max_rank\n",
    "        self.feat_norm = feat_norm\n",
    "        self.reranking = reranking\n",
    "\n",
    "    def reset(self):\n",
    "        self.feats = []\n",
    "        self.pids = []\n",
    "        self.camids = []\n",
    "\n",
    "    def update(self, output):  # called once for each batch\n",
    "        feat, pid, camid = output\n",
    "        self.feats.append(feat.cpu())\n",
    "        self.pids.extend(np.asarray(pid))\n",
    "        self.camids.extend(np.asarray(camid))\n",
    "\n",
    "    def compute(self):  # called after each epoch\n",
    "        feats = torch.cat(self.feats, dim=0)\n",
    "        if self.feat_norm:\n",
    "            print(\"The test feature is normalized\")\n",
    "            feats = torch.nn.functional.normalize(feats, dim=1, p=2)  # along channel\n",
    "        # query\n",
    "        qf = feats[:self.num_query]\n",
    "        q_pids = np.asarray(self.pids[:self.num_query])\n",
    "        q_camids = np.asarray(self.camids[:self.num_query])\n",
    "        # gallery\n",
    "        gf = feats[self.num_query:]\n",
    "        g_pids = np.asarray(self.pids[self.num_query:])\n",
    "\n",
    "        g_camids = np.asarray(self.camids[self.num_query:])\n",
    "        if self.reranking:\n",
    "            print('=> Enter reranking')\n",
    "            # distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)\n",
    "            distmat = re_ranking(qf, gf, k1=50, k2=15, lambda_value=0.3)\n",
    "\n",
    "        else:\n",
    "            print('=> Computing DistMat with euclidean_distance')\n",
    "            distmat = euclidean_distance(qf, gf)\n",
    "        cmc, mAP = eval_func(distmat, q_pids, g_pids, q_camids, g_camids)\n",
    "\n",
    "        return cmc, mAP, distmat, self.pids, self.camids, qf, gf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6c94b",
   "metadata": {
    "papermill": {
     "duration": 0.007564,
     "end_time": "2025-04-03T08:40:07.372568",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.365004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.6 Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df60725",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:07.388703Z",
     "iopub.status.busy": "2025-04-03T08:40:07.388509Z",
     "iopub.status.idle": "2025-04-03T08:40:11.546870Z",
     "shell.execute_reply": "2025-04-03T08:40:11.546219Z"
    },
    "papermill": {
     "duration": 4.168033,
     "end_time": "2025-04-03T08:40:11.548501",
     "exception": false,
     "start_time": "2025-04-03T08:40:07.380468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import argparse\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def load_detections(txt_file):\n",
    "    \"\"\" Load detections from a .txt file \"\"\"\n",
    "    detections = []\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        lines = f.readlines()[1:]  # Skip header\n",
    "        for line in lines:\n",
    "            cam_id, frame_id, class_id, x1, y1, x2, y2, score = line.strip().split(\",\")\n",
    "            frame_id = int(frame_id)\n",
    "            class_id = int(class_id)\n",
    "            bbox = (int(x1), int(y1), int(x2), int(y2))\n",
    "            score = float(score)\n",
    "            detections.append((frame_id, class_id, bbox, score))\n",
    "    return detections, cam_id\n",
    "\n",
    "def extract_person_images(video_path, detections):\n",
    "    \"\"\" Extract detected persons from video frames \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    person_data = []  # Store (frame_id, feature_vector)\n",
    "\n",
    "    for frame_id, class_id, bbox, score in detections:\n",
    "        if class_id != 0:  # Only process 'person' class (ID=0)\n",
    "            continue\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        person_crop = frame[y1:y2, x1:x2]\n",
    "\n",
    "        # Convert BGR to RGB and preprocess\n",
    "        image = Image.fromarray(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))\n",
    "        person_data.append((frame_id, preprocess_image(image)))\n",
    "\n",
    "    cap.release()\n",
    "    return person_data\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\" Preprocess image for ReID model \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image)\n",
    "\n",
    "def extract_features(model, person_data, device=\"cuda\"):\n",
    "    \"\"\" Extract features from person images using ReID model \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    features = []\n",
    "    frame_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for frame_id, img in person_data:\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            feat = model(img).cpu().numpy().flatten()\n",
    "            features.append(feat)\n",
    "            frame_ids.append(frame_id)\n",
    "\n",
    "    return np.array(frame_ids), np.array(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "458b4be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:11.565765Z",
     "iopub.status.busy": "2025-04-03T08:40:11.565446Z",
     "iopub.status.idle": "2025-04-03T08:40:11.569804Z",
     "shell.execute_reply": "2025-04-03T08:40:11.569203Z"
    },
    "papermill": {
     "duration": 0.013984,
     "end_time": "2025-04-03T08:40:11.570964",
     "exception": false,
     "start_time": "2025-04-03T08:40:11.556980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import torch\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import sys\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "\n",
    "# # Simulate command-line arguments in Jupyter Notebook\n",
    "# sys.argv = [\n",
    "#     \"notebook\",\n",
    "#     \"--load_detection\", \"/kaggle/input/dataset-detections/kaggle/working/output_text\",\n",
    "#     \"--video_path\", \"/kaggle/input/dataset-acc2024/scene_039\",\n",
    "#     \"--output_file\", \"/kaggle/working/output_features\",\n",
    "#     \"--ckpt_dir\", \"/kaggle/input/pose2id/pytorch/default/1/drive-download-20250327T073326Z-003\"\n",
    "# ]\n",
    "\n",
    "# # Argument parser\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--load_detection\", type=str, required=True, help=\"Path to detection text files\")\n",
    "# parser.add_argument(\"--video_path\", type=str, required=True, help=\"Path to video files\")\n",
    "# parser.add_argument(\"--output_file\", type=str, required=True, help=\"Path to save extracted features\")\n",
    "# parser.add_argument(\"--ckpt_dir\", type=str, required=True, help=\"Path to model checkpoint\")\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# # Ensure output directory exists\n",
    "# os.makedirs(args.output_file, exist_ok=True)\n",
    "\n",
    "# def load_detections(txt_file):\n",
    "#     \"\"\" Load detections from a .txt file in the format:\n",
    "#         cam_id, frame_id, person_id, x1, y1, x2, y2, score\n",
    "#     \"\"\"\n",
    "#     detections = []\n",
    "#     with open(txt_file, \"r\") as f:\n",
    "#         lines = f.readlines()[1:]  \n",
    "#         for line in lines:\n",
    "#             cam_id, frame_id, person_id, x1, y1, x2, y2, score = line.strip().split(\",\")\n",
    "            \n",
    "#             cam_id = cam_id\n",
    "#             frame_id = int(frame_id)\n",
    "#             person_id = int(person_id)\n",
    "#             bbox = (int(x1), int(y1), int(x2), int(y2))\n",
    "#             score = float(score)\n",
    "            \n",
    "#             # Append the detection information (frame_id, person_id, bbox, score)\n",
    "#             detections.append((frame_id, person_id, bbox, score))\n",
    "    \n",
    "#     return detections, cam_id\n",
    "\n",
    "# def extract_person_images(video_path, detections):\n",
    "#     \"\"\" Extract detected persons from video frames \"\"\"\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     if not cap.isOpened():\n",
    "#         print(f\"Error: Could not open video file {video_path}\")\n",
    "#         return []\n",
    "    \n",
    "#     person_data = []  # Store (frame_id, preprocessed_image)\n",
    "#     total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "#     for frame_id, person_id, bbox, score in detections:\n",
    "#         if frame_id >= total_frames:\n",
    "#             print(f\"Warning: Frame {frame_id} exceeds video length ({total_frames})\")\n",
    "#             continue\n",
    "\n",
    "#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             print(f\"Warning: Failed to read frame {frame_id}\")\n",
    "#             continue\n",
    "\n",
    "#         x1, y1, x2, y2 = bbox\n",
    "#         h, w = frame.shape[:2]\n",
    "#         x1, y1 = max(0, x1), max(0, y1)\n",
    "#         x2, y2 = min(w, x2), min(h, y2)\n",
    "        \n",
    "#         if x2 <= x1 or y2 <= y1:\n",
    "#             print(f\"Warning: Invalid bbox {bbox} for frame {frame_id}\")\n",
    "#             continue\n",
    "\n",
    "#         person_crop = frame[y1:y2, x1:x2]\n",
    "#         if person_crop.size == 0:\n",
    "#             print(f\"Warning: Empty crop for frame {frame_id}, bbox {bbox}\")\n",
    "#             continue\n",
    "\n",
    "#         image = Image.fromarray(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))\n",
    "#         person_data.append((frame_id, preprocess_image(image)))\n",
    "\n",
    "#     cap.release()\n",
    "#     print(f\"Processed {len(person_data)} person images from {video_path}\")\n",
    "#     return person_data\n",
    "\n",
    "# def preprocess_image(image):\n",
    "#     \"\"\" Preprocess image for ReID model \"\"\"\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.Resize((256, 128)),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#     ])\n",
    "#     return transform(image)\n",
    "\n",
    "# def extract_features(model, person_data, device=\"cuda\"):\n",
    "#     \"\"\" Extract features from person images using ReID model \"\"\"\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     features = []\n",
    "#     frame_ids = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for frame_id, img in person_data:\n",
    "#             img = img.unsqueeze(0).to(device)\n",
    "#             feat = model(img).cpu().numpy().flatten()\n",
    "#             features.append(feat)\n",
    "#             frame_ids.append(frame_id)\n",
    "\n",
    "#     return np.array(frame_ids), np.array(features)\n",
    "\n",
    "\n",
    "# cfg_transreid = pickle.load(open('/kaggle/input/cfg-trans-reid/cfg_transreid.pkl', 'rb'))\n",
    "# reid_net = make_model(cfg_transreid, num_class=751, camera_num=0, view_num=1)\n",
    "# reid_net.load_param(args.ckpt_dir + \"/transformer_20.pth\")\n",
    "\n",
    "\n",
    "# detection_dirs = [os.path.join(args.load_detection, d) for d in os.listdir(args.load_detection) if os.path.isdir(os.path.join(args.load_detection, d))]\n",
    "\n",
    "# for detection_dir in detection_dirs:\n",
    "#     detection_files = [os.path.join(detection_dir, f) for f in os.listdir(detection_dir) if f.endswith(\".txt\")]\n",
    "    \n",
    "#     for txt_file in detection_files:\n",
    "#         detections, cam_id = load_detections(txt_file)\n",
    "\n",
    "#         # Find matching video file\n",
    "#         video_file = os.path.join(args.video_path, f\"camera_{cam_id}\", \"video.mp4\")\n",
    "#         if not os.path.exists(video_file):\n",
    "#             print(f\"⚠️ No matching video found for {txt_file}. Skipping...\")\n",
    "#             continue\n",
    "\n",
    "#         print(f\"🔹 Processing Camera {cam_id}: {txt_file} with {video_file}\")\n",
    "\n",
    "#         # Extract person images\n",
    "#         person_data = extract_person_images(video_file, detections)\n",
    "\n",
    "#         # Extract features\n",
    "#         frame_ids, features = extract_features(reid_net, person_data)\n",
    "\n",
    "#         # Save features\n",
    "#         output_path = os.path.join(args.output_file, f\"features_cam{cam_id}.npy\")\n",
    "#         np.save(output_path, {\"frame_ids\": frame_ids, \"features\": features})\n",
    "\n",
    "#         print(f\"✅ Saved features for camera {cam_id} at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ccb329f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:11.587227Z",
     "iopub.status.busy": "2025-04-03T08:40:11.586985Z",
     "iopub.status.idle": "2025-04-03T08:40:11.589833Z",
     "shell.execute_reply": "2025-04-03T08:40:11.589206Z"
    },
    "papermill": {
     "duration": 0.012239,
     "end_time": "2025-04-03T08:40:11.590979",
     "exception": false,
     "start_time": "2025-04-03T08:40:11.578740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download_file('/kaggle/working/output_features', 'detections')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54fd75f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:11.607052Z",
     "iopub.status.busy": "2025-04-03T08:40:11.606846Z",
     "iopub.status.idle": "2025-04-03T08:40:11.612794Z",
     "shell.execute_reply": "2025-04-03T08:40:11.612171Z"
    },
    "papermill": {
     "duration": 0.015351,
     "end_time": "2025-04-03T08:40:11.614034",
     "exception": false,
     "start_time": "2025-04-03T08:40:11.598683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IDManager:\n",
    "    \"\"\"Handles global ID assignments and ensures consistency across cameras.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.id_map = {}  # Maps (cam_id, local_id) to global ID\n",
    "        self.reverse_map = {}  # Maps global ID to all assigned (cam_id, local_id) pairs\n",
    "        self.next_id = 1  # Next available global ID\n",
    "\n",
    "    def assign(self, cam_id, local_id, global_id=None):\n",
    "        \"\"\"Assigns a new global ID, ensuring consistency.\"\"\"\n",
    "        if (cam_id, local_id) in self.id_map:\n",
    "            return self.id_map[(cam_id, local_id)]  \n",
    "\n",
    "        if global_id is None:\n",
    "            global_id = self.next_id\n",
    "            self.next_id += 1 \n",
    "        self.id_map[(cam_id, local_id)] = global_id\n",
    "        self.reverse_map.setdefault(global_id, set()).add((cam_id, local_id))\n",
    "        \n",
    "        return global_id\n",
    "\n",
    "    def merge(self, cam_id1, local_id1, cam_id2, local_id2):\n",
    "        \"\"\"Ensures two people are merged under the same global ID.\"\"\"\n",
    "        id1 = self.id_map.get((cam_id1, local_id1))\n",
    "        id2 = self.id_map.get((cam_id2, local_id2))\n",
    "\n",
    "        if id1 and id2:\n",
    "            if id1 != id2:\n",
    "                for key in self.reverse_map[id2]:\n",
    "                    self.id_map[key] = id1\n",
    "                    self.reverse_map[id1].add(key)\n",
    "                del self.reverse_map[id2]\n",
    "        elif id1:\n",
    "            self.assign(cam_id2, local_id2, id1)\n",
    "        elif id2:\n",
    "            self.assign(cam_id1, local_id1, id2)\n",
    "        else:\n",
    "            new_id = self.assign(cam_id1, local_id1)\n",
    "            self.assign(cam_id2, local_id2, new_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a0b79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:11.630623Z",
     "iopub.status.busy": "2025-04-03T08:40:11.630424Z",
     "iopub.status.idle": "2025-04-03T08:40:11.636781Z",
     "shell.execute_reply": "2025-04-03T08:40:11.636206Z"
    },
    "papermill": {
     "duration": 0.015953,
     "end_time": "2025-04-03T08:40:11.637864",
     "exception": false,
     "start_time": "2025-04-03T08:40:11.621911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "def pairwise_distance(query_features, gallery_features):\n",
    "    x = query_features\n",
    "    y = gallery_features\n",
    "    m, n = x.size(0), y.size(0)\n",
    "    x = x.view(m, -1)\n",
    "    y = y.view(n, -1)\n",
    "    dist = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(m, n) + \\\n",
    "            torch.pow(y, 2).sum(dim=1, keepdim=True).expand(n, m).t()\n",
    "    dist.addmm_(1, -2, x, y.t())\n",
    "    return dist\n",
    "\n",
    "def NFC(feat: torch.Tensor, k1=2, k2=2):\n",
    "    feat = feat.clone()\n",
    "    dist = pairwise_distance(feat, feat)\n",
    "    \n",
    "    # Mask self-distance\n",
    "    eye = torch.eye(dist.size(0), device=dist.device)\n",
    "    dist = dist.masked_fill(eye == 1, float('inf'))\n",
    "    \n",
    "    # Find mutual nearest neighbors\n",
    "    val, rank = dist.topk(k1, largest=False)\n",
    "    mutual_topk_list = []\n",
    "    for i in range(rank.size(0)):\n",
    "        mutual_list = []\n",
    "        for j in rank[i]:\n",
    "            if i in rank[j][:k2]:\n",
    "                mutual_list.append(j.item())\n",
    "        mutual_topk_list.append(mutual_list)\n",
    "    \n",
    "    # Enhance features\n",
    "    feat_copy = feat.clone()\n",
    "    for i in range(rank.size(0)):\n",
    "        if mutual_topk_list[i]:\n",
    "            feat[i] += feat_copy[mutual_topk_list[i]].sum(dim=0)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18971854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:11.654141Z",
     "iopub.status.busy": "2025-04-03T08:40:11.653944Z",
     "iopub.status.idle": "2025-04-03T08:40:23.198370Z",
     "shell.execute_reply": "2025-04-03T08:40:23.197444Z"
    },
    "papermill": {
     "duration": 11.554394,
     "end_time": "2025-04-03T08:40:23.199952",
     "exception": false,
     "start_time": "2025-04-03T08:40:11.645558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-b1e89d398cd6>:10: UserWarning: This overload of addmm_ is deprecated:\n",
      "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta = 1, Number alpha = 1) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1642.)\n",
      "  dist.addmm_(1, -2, x, y.t())\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def load_features(cam_feature_paths, detections_dir):\n",
    "    \"\"\"Load and enhance features with NFC\"\"\"\n",
    "    cam_data = {}\n",
    "    for cam_path in cam_feature_paths:\n",
    "        cam_id = os.path.basename(cam_path).split(\"_cam\")[-1].split(\".\")[0]\n",
    "        data = np.load(cam_path, allow_pickle=True).item()\n",
    "        \n",
    "        # Load person IDs\n",
    "        detections_path = os.path.join(detections_dir, f\"camera_{cam_id}\", \"detections.txt\")\n",
    "        if not os.path.exists(detections_path):\n",
    "            continue\n",
    "        df = pd.read_csv(detections_path)\n",
    "        data[\"person_ids\"] = df[\"person_id\"].values\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        features = torch.FloatTensor(data[\"features\"])\n",
    "        features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "        \n",
    "        # Apply NFC on GPU if available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        with torch.no_grad():\n",
    "            enhanced = NFC(features.to(device))\n",
    "        \n",
    "        data[\"features\"] = torch.nn.functional.normalize(enhanced, p=2, dim=1).cpu().numpy()\n",
    "        cam_data[cam_id] = data\n",
    "    return cam_data\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def match_across_cameras(cam_data, threshold=0.92):\n",
    "    \"\"\"Matches people across cameras using cosine similarity and ID consistency.\"\"\"\n",
    "    id_manager = IDManager()\n",
    "    gallery = {}\n",
    "\n",
    "    avg_features = {}\n",
    "    for cam_id, data in cam_data.items():\n",
    "        person_feats = {}\n",
    "        for pid, feat in zip(data[\"person_ids\"], data[\"features\"]):\n",
    "            person_feats.setdefault(pid, []).append(feat)\n",
    "        avg_features[cam_id] = {pid: np.mean(feats, axis=0) for pid, feats in person_feats.items()}\n",
    "\n",
    "    for cam_id, persons in avg_features.items():\n",
    "        for pid, feat in persons.items():\n",
    "            best_match = None\n",
    "            max_similarity = -1  \n",
    "\n",
    "            for gid, gfeat in gallery.items():\n",
    "                similarity = cosine_similarity(feat, gfeat)\n",
    "                if similarity > max_similarity and similarity > threshold:\n",
    "                    max_similarity = similarity\n",
    "                    best_match = gid\n",
    "\n",
    "            if best_match is not None:\n",
    "                id_manager.assign(cam_id, pid, best_match)\n",
    "            else:\n",
    "                new_id = id_manager.assign(cam_id, pid)\n",
    "                gallery[new_id] = feat\n",
    "\n",
    "    return id_manager.id_map  \n",
    "\n",
    "def update_detections(person_mapping, detections_path, output_path):\n",
    "    \"\"\"Update detection files with global IDs\"\"\"\n",
    "    df = pd.read_csv(detections_path)\n",
    "    df[\"person_id\"] = df.apply(lambda row: person_mapping.get(\n",
    "        (str(row[\"cam_id\"]), row[\"person_id\"]), row[\"person_id\"]), axis=1)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cam_feature_paths = [\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0342.npy\",\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0343.npy\",\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0344.npy\",\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0345.npy\",\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0346.npy\",\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0347.npy\",\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0348.npy\",\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0349.npy\",\n",
    "        \"/kaggle/input/reid-feats/kaggle/working/output_features/features_cam0350.npy\"\n",
    "    ]\n",
    "    detections_dir = \"/kaggle/input/dataset-detections/kaggle/working/output_text\"\n",
    "    output_dir = \"/kaggle/working/output_text\"\n",
    "\n",
    "    cam_data = load_features(cam_feature_paths, detections_dir)\n",
    "    person_mapping = match_across_cameras(cam_data)\n",
    "    \n",
    "    for cam_id in cam_data:\n",
    "        det_path = os.path.join(detections_dir, f\"camera_{cam_id}\", \"detections.txt\")\n",
    "        out_path = os.path.join(output_dir, f\"camera_{cam_id}\", \"global_detections.txt\")\n",
    "        update_detections(person_mapping, det_path, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "925414d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:23.217057Z",
     "iopub.status.busy": "2025-04-03T08:40:23.216782Z",
     "iopub.status.idle": "2025-04-03T08:40:23.291443Z",
     "shell.execute_reply": "2025-04-03T08:40:23.290790Z"
    },
    "papermill": {
     "duration": 0.084293,
     "end_time": "2025-04-03T08:40:23.292587",
     "exception": false,
     "start_time": "2025-04-03T08:40:23.208294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='matching.zip' target='_blank'>matching.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/matching.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_file('/kaggle/working/output_text', 'matching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "299bc67d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:40:23.309501Z",
     "iopub.status.busy": "2025-04-03T08:40:23.309281Z",
     "iopub.status.idle": "2025-04-03T08:41:15.395406Z",
     "shell.execute_reply": "2025-04-03T08:41:15.394259Z"
    },
    "papermill": {
     "duration": 52.096202,
     "end_time": "2025-04-03T08:41:15.396897",
     "exception": false,
     "start_time": "2025-04-03T08:40:23.300695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0342/annotated_video.mp4\n",
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0343/annotated_video.mp4\n",
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0344/annotated_video.mp4\n",
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0345/annotated_video.mp4\n",
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0346/annotated_video.mp4\n",
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0347/annotated_video.mp4\n",
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0348/annotated_video.mp4\n",
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0349/annotated_video.mp4\n",
      "⏩ Processed 50 frames\n",
      "⏩ Processed 100 frames\n",
      "⏩ Processed 150 frames\n",
      "⏩ Processed 200 frames\n",
      "⏩ Processed 250 frames\n",
      "⏩ Processed 300 frames\n",
      "✅ Completed: /kaggle/working/output_video/camera_0350/annotated_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def draw_detections(video_path, detections_path, output_video=None, max_frames=300):\n",
    "    \"\"\"Safe video annotation for Kaggle kernels\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"⚠️ Video open failed: {video_path}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(detections_path, sep=',')\n",
    "    df.columns = df.columns.str.strip().str.lower()  \n",
    "    \n",
    "    required_columns = {'frame_id', 'x1', 'y1', 'x2', 'y2'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        missing = required_columns - set(df.columns)\n",
    "        print(f\"❌ Missing columns: {missing}\")\n",
    "        return\n",
    "\n",
    "    # Video writer setup\n",
    "    writer = None\n",
    "    if output_video:\n",
    "        os.makedirs(os.path.dirname(output_video), exist_ok=True)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        writer = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    batch_size = 50  # Process in chunks to manage memory\n",
    "\n",
    "    while frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        detections = df[df[\"frame_id\"] == frame_count]\n",
    "        \n",
    "        # Draw boxes\n",
    "        for _, det in detections.iterrows():\n",
    "            x1, y1, x2, y2 = map(int, det[['x1', 'y1', 'x2', 'y2']])\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            if 'person_id' in df.columns and 'score' in df.columns:\n",
    "                label = f\"{det['person_id']} ({det['score']:.2f})\"\n",
    "                cv2.putText(frame, label, (x1, y1-10), \n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 1)\n",
    "\n",
    "        # Write frame without display\n",
    "        if writer is not None:\n",
    "            writer.write(frame)\n",
    "\n",
    "        frame_count += 1\n",
    "        \n",
    "        # Periodic cleanup\n",
    "        if frame_count % batch_size == 0:\n",
    "            print(f\"⏩ Processed {frame_count} frames\")\n",
    "            \n",
    "    cap.release()\n",
    "    if writer is not None:\n",
    "        writer.release()\n",
    "    print(f\"✅ Completed: {output_video or 'No output'}\")\n",
    "\n",
    "for cam_id in range(342, 351):  \n",
    "    camera_name = f\"camera_{cam_id:04d}\" \n",
    "    video_path = f\"/kaggle/input/dataset-acc2024/scene_039/{camera_name}/video.mp4\"\n",
    "    detections_path = f\"/kaggle/input/global-detection5/kaggle/working/output_text/{camera_name}/global_detections.txt\"\n",
    "    output_video = f\"/kaggle/working/output_video/{camera_name}/annotated_video.mp4\"\n",
    "\n",
    "    draw_detections(video_path, detections_path, output_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e7d3a5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T08:41:15.419318Z",
     "iopub.status.busy": "2025-04-03T08:41:15.419008Z",
     "iopub.status.idle": "2025-04-03T08:41:20.545296Z",
     "shell.execute_reply": "2025-04-03T08:41:20.544441Z"
    },
    "papermill": {
     "duration": 5.138412,
     "end_time": "2025-04-03T08:41:20.546684",
     "exception": false,
     "start_time": "2025-04-03T08:41:15.408272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='matching_videos.zip' target='_blank'>matching_videos.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/matching_videos.zip"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_file('/kaggle/working/output_video', 'matching_videos')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6961927,
     "sourceId": 11157882,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6978971,
     "sourceId": 11181055,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6979095,
     "sourceId": 11181246,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6979547,
     "sourceId": 11181825,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6979832,
     "sourceId": 11182176,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6980337,
     "sourceId": 11182835,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6987329,
     "sourceId": 11192633,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7020620,
     "sourceId": 11237783,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7027737,
     "sourceId": 11247210,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7036607,
     "sourceId": 11258917,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7038072,
     "sourceId": 11260835,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7038164,
     "sourceId": 11260961,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7038212,
     "sourceId": 11261020,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 277352,
     "modelInstanceId": 256035,
     "sourceId": 299634,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 280703,
     "modelInstanceId": 259522,
     "sourceId": 304052,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 280779,
     "modelInstanceId": 259598,
     "sourceId": 304157,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 115.517743,
   "end_time": "2025-04-03T08:41:21.677236",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-03T08:39:26.159493",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
